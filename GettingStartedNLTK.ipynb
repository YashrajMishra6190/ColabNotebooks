{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbum+pa5CW5a5YCmuaJHyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashrajMishra6190/ColabNotebooks/blob/main/GettingStartedNLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to NLTK"
      ],
      "metadata": {
        "id": "PdvyvxSl7ugX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG6rfDeg3YSB",
        "outputId": "b94cfb73-4a17-4dcc-e60b-c6e4bd6a23bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Hello Everyone. We're hoping you guys are doing great.\""
      ],
      "metadata": {
        "id": "tSRgj-cL3vGi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MZjqlBcD3vDM",
        "outputId": "08b7bf7a-1509-4670-82ce-8233e5b6fe04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Everyone. We're hoping you guys are doing great.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2HowQkG3vAz",
        "outputId": "16b83830-5b3f-4fcc-8cdd-450c2ea9104b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u74Kb67H3uyp",
        "outputId": "081a2ea8-29ba-4db4-9b05-d94e8d66fa87"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Everyone', \" We're hoping you guys are doing great\", '']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(txt.split(' '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNZxneS26E2k",
        "outputId": "16f03368-3c9e-48f6-9607-98b79bc42fd3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHfEgkdx6EzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(txt.split('.')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_uQ1x0Q4Y17",
        "outputId": "33183518-b371-4e49-d298-e3b7e0edf241"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now here is problem arises as we can see there is only 2 sentences.\n",
        "#importing in NLTK: Natural language toolkit\n",
        "\n",
        "#If there is any error to import => nltk.download('punkt')\n",
        "import nltk\n",
        "\n",
        "#There are two types of tokens present in the NLTK\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "YZqTbPe54Y36"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z_oMa9x4Y63",
        "outputId": "e7a96c4f-c1fd-4fc9-fd48-153e7ba307c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_tokenize(txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhvOyBM24Y-M",
        "outputId": "3e006d05-043e-432e-f841-6440360ae3b8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets iterate\n",
        "for i in word_tokenize(txt):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhk06mT-4ZEY",
        "outputId": "cc76af44-02ce-4347-c0ef-089f02556326"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Everyone\n",
            ".\n",
            "We\n",
            "'re\n",
            "hoping\n",
            "you\n",
            "guys\n",
            "are\n",
            "doing\n",
            "great\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_tokenize(txt):\n",
        "  if (i !='.'):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9QXTzTQ4ZGb",
        "outputId": "7c8a160c-7442-4f21-f015-fae6553aef75"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Everyone\n",
            "We\n",
            "'re\n",
            "hoping\n",
            "you\n",
            "guys\n",
            "are\n",
            "doing\n",
            "great\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in sent_tokenize(txt):\n",
        "  print(j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJJNFD_c4ZIv",
        "outputId": "804e7ee4-13e3-4123-b5ca-2373b441e1e1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Everyone.\n",
            "We're hoping you guys are doing great.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here as you can see there is only two sentence output but in the traditional techniques there was 3 sentence output\n",
        "len(sent_tokenize(txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o63nLBKU4ZMF",
        "outputId": "db8f193c-6106-4ea3-fcf1-1f1a7ab6d4ae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and Lematization\n",
        "---\n",
        "These are two techniques in natural language processing which is used to reduce vocab size {how many unique words we are having in the whole string}"
      ],
      "metadata": {
        "id": "_Z5VhRY67tn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp88lafy72Zd",
        "outputId": "f13265f9-5cbd-4d61-c022-f0a330c66946"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#therefore\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "stem = PorterStemmer()\n",
        "lam = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ElbtyUHx8t7_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization : Makes Sense\n",
        "print(lam.lemmatize('change'))\n",
        "print(lam.lemmatize('changes'))\n",
        "print(lam.lemmatize('changer'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL90PIlc8tvB",
        "outputId": "cc647881-cfb6-462f-d5c9-d69da74cade4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change\n",
            "change\n",
            "changer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lam.lemmatize('chocolate'))\n",
        "print(lam.lemmatize('choco'))\n",
        "print(lam.lemmatize('chocolates'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6DhHHHA9nCi",
        "outputId": "b265680e-52e3-49b7-92f9-419e1bba52a9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chocolate\n",
            "choco\n",
            "chocolate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemmer: Not makes sense\n",
        "print(stem.stem('retrieved'))\n",
        "print(stem.stem('retrieves'))\n",
        "print(stem.stem('retrieval'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfQdpcqq9m_L",
        "outputId": "c192bfc6-00f0-4560-f404-b951e35d86db"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retriev\n",
            "retriev\n",
            "retriev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stem.stem('change'))\n",
        "print(stem.stem('changes'))\n",
        "print(stem.stem('changer'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKt9yqrm-Sl4",
        "outputId": "2a79fb6d-69ce-4006-b15d-066ee8a8fd16"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chang\n",
            "chang\n",
            "changer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words"
      ],
      "metadata": {
        "id": "Xerfwn7x-hV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDMfbQ4r-5BI",
        "outputId": "a586b593-4858-4b1d-fcea-b5054263bb0c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "#List of the stop words -> stopwords.words()\n",
        "stopword = stopwords.words('english')"
      ],
      "metadata": {
        "id": "R7uz1C8d-lTg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mirNGWfx_VXz",
        "outputId": "c8671ddb-6935-4fe6-a4ae-c43bc0ba8b04"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Length of the stopwords in ennglish : 179\n",
        "len(stopword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAykPMS0_VUb",
        "outputId": "59107504-0cd3-484f-c51a-577d94f7569f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"This is a good time to talk\""
      ],
      "metadata": {
        "id": "DxHUMMVT_VLD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "txt = word_tokenize(txt)\n",
        "for word in txt:\n",
        "  print(word, word in stopword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_ayZnaP_VHx",
        "outputId": "c1b76bf4-6589-4599-e2b8-433ebbde12a0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This False\n",
            "is True\n",
            "a True\n",
            "good False\n",
            "time False\n",
            "to True\n",
            "talk False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in txt:\n",
        "  if(word.lower() not in stopword) and (len(word) != 1):\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMiOd2XL_VFZ",
        "outputId": "781a89d4-7fd6-4168-b4f3-ec3f44369ee3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "time\n",
            "talk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus and Vocabulary"
      ],
      "metadata": {
        "id": "1yYU75zuAZWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"India, officially the Republic of India is a country in South Asia. It is the seventh-largest country by area; the most populous country and the world's most populous democracy. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\""
      ],
      "metadata": {
        "id": "Qpk4JuWIAYHr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "z4j-nO1JAYET",
        "outputId": "e33137a5-d79d-4e3a-8032-115d5ba1d17f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"India, officially the Republic of India is a country in South Asia. It is the seventh-largest country by area; the most populous country and the world's most populous democracy. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "AzPASyU9AYB7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BfQAujSBIAh",
        "outputId": "32277a2b-d935-4a27-e06d-0bdafb4f5fbb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['India',\n",
              " ',',\n",
              " 'officially',\n",
              " 'the',\n",
              " 'Republic',\n",
              " 'of',\n",
              " 'India',\n",
              " 'is',\n",
              " 'a',\n",
              " 'country',\n",
              " 'in',\n",
              " 'South',\n",
              " 'Asia',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'the',\n",
              " 'seventh-largest',\n",
              " 'country',\n",
              " 'by',\n",
              " 'area',\n",
              " ';',\n",
              " 'the',\n",
              " 'most',\n",
              " 'populous',\n",
              " 'country',\n",
              " 'and',\n",
              " 'the',\n",
              " 'world',\n",
              " \"'s\",\n",
              " 'most',\n",
              " 'populous',\n",
              " 'democracy',\n",
              " '.',\n",
              " 'Bounded',\n",
              " 'by',\n",
              " 'the',\n",
              " 'Indian',\n",
              " 'Ocean',\n",
              " 'on',\n",
              " 'the',\n",
              " 'south',\n",
              " ',',\n",
              " 'the',\n",
              " 'Arabian',\n",
              " 'Sea',\n",
              " 'on',\n",
              " 'the',\n",
              " 'southwest',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'Bay',\n",
              " 'of',\n",
              " 'Bengal',\n",
              " 'on',\n",
              " 'the',\n",
              " 'southeast',\n",
              " ',',\n",
              " 'it',\n",
              " 'shares',\n",
              " 'land',\n",
              " 'borders',\n",
              " 'with',\n",
              " 'Pakistan',\n",
              " 'to',\n",
              " 'the',\n",
              " 'west',\n",
              " ';',\n",
              " 'China',\n",
              " ',',\n",
              " 'Nepal',\n",
              " ',',\n",
              " 'and',\n",
              " 'Bhutan',\n",
              " 'to',\n",
              " 'the',\n",
              " 'north',\n",
              " ';',\n",
              " 'and',\n",
              " 'Bangladesh',\n",
              " 'and',\n",
              " 'Myanmar',\n",
              " 'to',\n",
              " 'the',\n",
              " 'east',\n",
              " '.',\n",
              " 'In',\n",
              " 'the',\n",
              " 'Indian',\n",
              " 'Ocean',\n",
              " ',',\n",
              " 'India',\n",
              " 'is',\n",
              " 'in',\n",
              " 'the',\n",
              " 'vicinity',\n",
              " 'of',\n",
              " 'Sri',\n",
              " 'Lanka',\n",
              " 'and',\n",
              " 'the',\n",
              " 'Maldives',\n",
              " ';',\n",
              " 'its',\n",
              " 'Andaman',\n",
              " 'and',\n",
              " 'Nicobar',\n",
              " 'Islands',\n",
              " 'share',\n",
              " 'a',\n",
              " 'maritime',\n",
              " 'border',\n",
              " 'with',\n",
              " 'Thailand',\n",
              " ',',\n",
              " 'Myanmar',\n",
              " ',',\n",
              " 'and',\n",
              " 'Indonesia',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Word removal Technique"
      ],
      "metadata": {
        "id": "depHZD5dDzHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unique words : size of vocablary\n",
        "words = []\n",
        "\n",
        "stopword = stopwords.words('english')\n",
        "for word in word_tokenize(corpus):\n",
        "  if(word.lower() not in stopword) and len(word) >= 2:\n",
        "    words.append(word.lower())"
      ],
      "metadata": {
        "id": "3M1EVq6uCCXQ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP56SAOqCCUg",
        "outputId": "262c86ba-e74e-4f0e-a5de-52ca43a90459"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['india',\n",
              " 'officially',\n",
              " 'republic',\n",
              " 'india',\n",
              " 'country',\n",
              " 'south',\n",
              " 'asia',\n",
              " 'seventh-largest',\n",
              " 'country',\n",
              " 'area',\n",
              " 'populous',\n",
              " 'country',\n",
              " 'world',\n",
              " \"'s\",\n",
              " 'populous',\n",
              " 'democracy',\n",
              " 'bounded',\n",
              " 'indian',\n",
              " 'ocean',\n",
              " 'south',\n",
              " 'arabian',\n",
              " 'sea',\n",
              " 'southwest',\n",
              " 'bay',\n",
              " 'bengal',\n",
              " 'southeast',\n",
              " 'shares',\n",
              " 'land',\n",
              " 'borders',\n",
              " 'pakistan',\n",
              " 'west',\n",
              " 'china',\n",
              " 'nepal',\n",
              " 'bhutan',\n",
              " 'north',\n",
              " 'bangladesh',\n",
              " 'myanmar',\n",
              " 'east',\n",
              " 'indian',\n",
              " 'ocean',\n",
              " 'india',\n",
              " 'vicinity',\n",
              " 'sri',\n",
              " 'lanka',\n",
              " 'maldives',\n",
              " 'andaman',\n",
              " 'nicobar',\n",
              " 'islands',\n",
              " 'share',\n",
              " 'maritime',\n",
              " 'border',\n",
              " 'thailand',\n",
              " 'myanmar',\n",
              " 'indonesia']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4MXbI69EZzd",
        "outputId": "ece56474-eec4-4483-a176-e56efdf92775"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, converting list to the set\n",
        "#Set is a data structure which only takes unique value\n",
        "set(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcRa7gIYCCRr",
        "outputId": "9637a484-dc92-4834-c337-618e62b40cae"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'s\",\n",
              " 'andaman',\n",
              " 'arabian',\n",
              " 'area',\n",
              " 'asia',\n",
              " 'bangladesh',\n",
              " 'bay',\n",
              " 'bengal',\n",
              " 'bhutan',\n",
              " 'border',\n",
              " 'borders',\n",
              " 'bounded',\n",
              " 'china',\n",
              " 'country',\n",
              " 'democracy',\n",
              " 'east',\n",
              " 'india',\n",
              " 'indian',\n",
              " 'indonesia',\n",
              " 'islands',\n",
              " 'land',\n",
              " 'lanka',\n",
              " 'maldives',\n",
              " 'maritime',\n",
              " 'myanmar',\n",
              " 'nepal',\n",
              " 'nicobar',\n",
              " 'north',\n",
              " 'ocean',\n",
              " 'officially',\n",
              " 'pakistan',\n",
              " 'populous',\n",
              " 'republic',\n",
              " 'sea',\n",
              " 'seventh-largest',\n",
              " 'share',\n",
              " 'shares',\n",
              " 'south',\n",
              " 'southeast',\n",
              " 'southwest',\n",
              " 'sri',\n",
              " 'thailand',\n",
              " 'vicinity',\n",
              " 'west',\n",
              " 'world'}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06K9KDbuCCPM",
        "outputId": "2a734bc1-19b3-429e-a95a-58a1e756c4f0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1EMqPaxCCM1",
        "outputId": "5f1817df-f0d8-49ce-e6fe-3399002450bc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(words))\n",
        "vocab#Processed vocab size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3zEIX6rFE5i",
        "outputId": "6a075c7b-6301-4fb8-9f1d-de3f856b3baa"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bay',\n",
              " 'nepal',\n",
              " 'democracy',\n",
              " 'country',\n",
              " 'india',\n",
              " 'maldives',\n",
              " 'islands',\n",
              " 'indonesia',\n",
              " 'bhutan',\n",
              " 'pakistan',\n",
              " 'bengal',\n",
              " 'republic',\n",
              " 'andaman',\n",
              " 'arabian',\n",
              " 'populous',\n",
              " 'nicobar',\n",
              " 'shares',\n",
              " 'north',\n",
              " 'seventh-largest',\n",
              " 'west',\n",
              " 'china',\n",
              " \"'s\",\n",
              " 'vicinity',\n",
              " 'ocean',\n",
              " 'border',\n",
              " 'east',\n",
              " 'share',\n",
              " 'sea',\n",
              " 'world',\n",
              " 'thailand',\n",
              " 'asia',\n",
              " 'land',\n",
              " 'indian',\n",
              " 'bangladesh',\n",
              " 'officially',\n",
              " 'borders',\n",
              " 'southwest',\n",
              " 'south',\n",
              " 'myanmar',\n",
              " 'sri',\n",
              " 'lanka',\n",
              " 'maritime',\n",
              " 'bounded',\n",
              " 'southeast',\n",
              " 'area']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now take each every sentence then encode the text, So whenever you are working on the some type of text in machine learning or deep learning library then `first convet text data -> into numbers`"
      ],
      "metadata": {
        "id": "S30UuxV1Ghvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in sent_tokenize(corpus):\n",
        "  print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIhrL6HPFE2W",
        "outputId": "27c9fcf1-4bb0-487e-ffe4-f330192d516f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India, officially the Republic of India is a country in South Asia.\n",
            "It is the seventh-largest country by area; the most populous country and the world's most populous democracy.\n",
            "Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.\n",
            "In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding techniques\n",
        "'this is a car'\n",
        "# this = 1\n",
        "# is = 2\n",
        "# a = 3\n",
        "# car = 4\n",
        "\n",
        "# 1 2 3 4 {this is a car}\n",
        "# 2 1 3 4 {is this a car}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tw-n72DCFE0M",
        "outputId": "244b5bb7-69fa-4178-f961-5ec86b278121"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a car'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab) # vocab size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLxF--yHFExj",
        "outputId": "beca825a-2954-4763-f161-4dedad091151"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocab with Keras"
      ],
      "metadata": {
        "id": "nWfqliwuHkUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8HogiSmHpIK",
        "outputId": "f4cc5665-fcf9-4d30-9144-fffb79a27936"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text to Sequences"
      ],
      "metadata": {
        "id": "aKvJTT6QKxqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "DivNN4ibH4xO"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()"
      ],
      "metadata": {
        "id": "kF0rfQ8sH4t4"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corp = ['Coffee is hot','water is cold']\n",
        "#Tokenizer is passed in the dataset-> corpus\n",
        "tok.fit_on_texts(corp)"
      ],
      "metadata": {
        "id": "ERhayeClIVrM"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding format\n",
        "tok.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeADVLsxIVn9",
        "outputId": "7cbdd0d4-7943-4cd3-e2da-f2800aa0c6da"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 1, 'coffee': 2, 'hot': 3, 'water': 4, 'cold': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok.texts_to_sequences(corp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvFwlmB9IVlR",
        "outputId": "39eeafbe-de5b-4ded-c08e-d014090ba192"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1, 3], [4, 1, 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If there is no word there in a encoding format then it will be neglected like,\n",
        "tok.texts_to_sequences(['black Coffee is hot','water is cold'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUUPu9MBIViz",
        "outputId": "a0a94125-00cf-49d6-e05b-10e8db0a7030"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1, 3], [4, 1, 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding OVV"
      ],
      "metadata": {
        "id": "ZC8HA8oLKnP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok = Tokenizer(oov_token = 'black')\n",
        "corp =  ['coffee is hot','water is cold']\n",
        "\n",
        "tok.fit_on_texts(corp)"
      ],
      "metadata": {
        "id": "KNoBguUqJtxE"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7UK3GkhJttv",
        "outputId": "c20ffd00-3579-4352-cee6-fd29b90bb484"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'black': 1, 'is': 2, 'coffee': 3, 'hot': 4, 'water': 5, 'cold': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok.texts_to_sequences(['black Coffee is hot','water is cold'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxgGuEPKKNVR",
        "outputId": "c0d2fe06-65f9-42d4-efd3-c3979232614d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 3, 2, 4], [5, 2, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limiting the Number of words"
      ],
      "metadata": {
        "id": "jOIakZwJKqxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok = Tokenizer(oov_token = 'black')\n",
        "corp =  ['coffee is hot','water is cold']\n",
        "\n",
        "tok.fit_on_texts(corp)\n",
        "print(tok.word_index)\n",
        "\n",
        "tok.texts_to_sequences(['black coffee is cold','water is hot'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTDlYz4BKc_k",
        "outputId": "fd572502-c31f-4204-e27e-aeb46ac51e8e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'black': 1, 'is': 2, 'coffee': 3, 'hot': 4, 'water': 5, 'cold': 6}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 3, 2, 6], [5, 2, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    }
  ]
}